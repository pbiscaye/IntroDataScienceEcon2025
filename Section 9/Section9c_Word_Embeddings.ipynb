{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c0c2f15-1a64-4b2b-8ca1-270f05aa24b9",
   "metadata": {},
   "source": [
    "# Section 9. Word Embeddings\n",
    "\n",
    "#### Instructor: Pierre Biscaye \n",
    "\n",
    "This is the third of three notebooks covering the foundations for performing **text analysis** in Python. In the previous parts, we learned how to perform text preprocessing and convert text into numeric representations with Bags of Words (BoW) and TF-IDF. These methods make heavy use of word frequency but not much of the relative positions between words, but there's still rich semantic (relating to language or logic) and syntactic (relating to structure) meanings left to be captured beyond independent frequencies of words. \n",
    "\n",
    "We need a more powerful tool that has the potential to represent rich semantics (and more) of our text data. In the final part of this series, we will dive into word embeddings, a method widely combined with more advanced Natural Language Processing (NLP) tasks. We'll make extensive use of the `gensim` package, which hosts a range of word embedding models, including `word2vec` and `glove`, the two models we'll explore today.\n",
    "\n",
    "The content of this notebook is taken from UC Berkeley D-Lab's Python Text Analysis [course](https://github.com/dlab-berkeley/Python-Text-Analysis).\n",
    "\n",
    "### Sections\n",
    "1. Understand word embeddings\n",
    "2. Word similarity and word analogy\n",
    "3. Semantic axes and associations in word embeddings\n",
    "4. Bringing it together: tweet sentiment prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb1d3b7-bf13-4a9e-ba89-aed7eb0603d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run if you do not have gensim installed\n",
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0add7f3-98c6-4248-bed3-1d4c5e0f6d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fc6996",
   "metadata": {},
   "source": [
    "# 1. Understand Word Emebeddings \n",
    "\n",
    "As famously put by British Linguist J.R. Firth:\n",
    "\n",
    "> **You shall know a word by the company it keeps.**\n",
    "\n",
    "This quote summarizes the essence of word embeddings, which take the numerical representation of text a step further to consider word context. \n",
    "\n",
    "Recall from notebook 9b that a BoW representation is a **sparse** matrix. Its dimension is determined by vocabulary size and the number of documents. Importantly, a sparse matrix like BoW is interpretable: the cell values refer to the count of a word in a document. Oftentimes the cell values are zeros: many words do not simply appear in a particular document. \n",
    "\n",
    "We can think of word embedding as a matrix likewise, but this time a **dense** matrix, where the cell values are real numbers. Word embeddings project a word's meaning onto a high-dimensional vector space, which is why it is also called **word vectors**. A word vector is essentially an array of real numbers, the length of which, as we'll see today, could be as low as 50, or as high as 300 (or even higher in Large Language Models). These real number vectors do not make explicit sense to us, but capture aspects of semantic or syntactic meanings of words.  \n",
    "\n",
    "BOW:\n",
    "- Sparse matrix\n",
    "- Dimension: $D$ x $V$, where rows are **D**ocuments and columns are words in the **V**ocabulary.\n",
    "- Interpretable: e.g., in a financial document, \"bank\" and \"banker\" could appear a lot of times but not \"bang\".\n",
    "\n",
    "<img src='Images/bow-illustration-2.png' alt=\"BoW\" width=\"500\">\n",
    "\n",
    "Word embeddings:\n",
    "- Dense matrix\n",
    "- Dimension: $V$ x $D$, where rows are **V**ocabulary and columns are vectors with dimension **D**.\n",
    "- Not immediately interpretable\n",
    "\n",
    "<img src='Images/bow-illustration-3.png' alt=\"BoW\" width=\"500\">\n",
    "\n",
    "Today, we are going to explore two widely used word embedding models, `word2vec` and `glove`. We will use the package `gensim` to access both models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b2ac31-37a4-463d-8f49-bfd93c42904c",
   "metadata": {},
   "source": [
    "## `word2vec`\n",
    "\n",
    "Before diving into `word2vec`, let's talk a bit of history first. The idea of word vectors, i.e, projecting a word's meaning onto a vector space, has been around for a long time. The `word2vec` model, proposed by [Mikolov et al.](https://arxiv.org/abs/1310.4546) in 2013, introduces an efficient model of word embeddings. Since then it has stimulated a new wave of research into this topic. \n",
    "\n",
    "The key question asked in the 2013 paper is: how do we go about learning a good vector representation from the data?\n",
    "\n",
    "Mikolov et al. proposed two approaches: the **continuous bag-of-words (CBOW)** and the **skip-gram (SG)**. Both are similar in that we use the vector representation of a token to try and predict what the nearby tokens are with a shallow neural network.   \n",
    "\n",
    "Take the following sentence for example. If our target token is $w_t$, \"banks\", the context tokens would be the preceding tokens $w_{t-2}, w_{t-1}$ and the following ones $w_{t+1}, w_{t+2}$. This corresponds to a **window size** of 2: 2 words on either side of the target word. Similarly when we move onto the next target token, the context window (tokens underlined) moves as well.\n",
    "\n",
    "<img src='Images/target_word.png' alt=\"Target word\" width=\"500\">\n",
    "\n",
    "In the continuous bag-of-words model, the goal is to predict the target token, given the context tokens. In the skip-gram model, the task is to predict the context tokens from the target token. This is the reverse of the continuous bag-of-words, and is a harder task, since we have to predict more from less information.\n",
    "\n",
    "<img src='Images/word2vec-model.png' alt=\"word2vec\" width=\"550\">\n",
    "\n",
    "**CBOW** (Left):\n",
    "- **Input**: context tokens\n",
    "- **Inner dimension**: embedding layer\n",
    "- **Output**: the target token\n",
    "\n",
    "**Skip-gram** (Right):\n",
    "- **Input**: the target token\n",
    "- **Inner dimension**: embedding layer\n",
    "- **Output**: context tokens\n",
    "\n",
    "The above figure illustrates the direction of prediction. It also serves as a schematic representation of a neural network, i.e., the mechanism underlying the training of `word2vec`. The input and output are known to us, represented by **one-hot encodings** in Mikolov et al. The **hidden layer**, the inner dimension in-between the input and the output, is the vector representation that we are trying to find out. \n",
    "\n",
    "We won't go into the specifics of training but provide a brief idea of where does embedding come from. The `word2vec` model we will be interacting with today is **pretrained**, meaning that the embeddings have already been trained on a large corpus (or a number of corpora). The pretrained `word2vec` and `glove`, as well as other models, are available through `gensim`. \n",
    "\n",
    "Let's take a look at a few word embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a320e320-8a4e-4cf9-8d14-378442fe5845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word embedding models\n",
    "gensim_models = list(api.info()['models'].keys())\n",
    "\n",
    "for model in gensim_models:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c508421b-dd32-4173-8716-b47f89cd1a51",
   "metadata": {},
   "source": [
    "Consider the one named `word2vec-google-news-300`. The model name is usually formatted as `model-corpora-dimension`, so this is a `word2vec` model that is trained on Google News, and the embedding has 300 dimensions. \n",
    "\n",
    "We can retrieve this model in two ways:\n",
    "- Downloading it via `api.load()`\n",
    "- Downloading the model as a zip file beforehand and then loading it in with `KeyedVectors.load()`\n",
    "\n",
    "The pretrained word2vec is archived by Google, and is available to download via this [link](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6a28b0-475c-401a-a744-2d67a0c51d61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the following line if your local machine has plenty of memory\n",
    "#wv = api.load(\"word2vec-google-news-300\")\n",
    "# Alternatively, load the model in (https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/view?resourcekey=0-wjGZdNAUop6WykTtMip30g)\n",
    "wv = KeyedVectors.load_word2vec_format('Data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "# The parameter `binary` asks whether the model is in the binary format (indicated by the extension `.bin`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925db1ea-6295-47b7-b6f2-45941c993e5c",
   "metadata": {},
   "source": [
    "Accessing the actual word vectors can be done by treating the word vector model as a dictionary. \n",
    "\n",
    "For example, let's take a look at the word vector for \"banana\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93fb02c-62ba-42f1-b6cb-b492bd21516e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wv['banana']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf15e57-7c49-4238-849b-90cc30813d2e",
   "metadata": {},
   "source": [
    "We can take a look at the shape of the \"banana\" vector. As promised, it is an 1-D array that holds 300 values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9c61de-8898-418b-a469-b25baa1e29b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv['banana'].size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1de233-cef4-48c8-9f5d-57107a16d02d",
   "metadata": {},
   "source": [
    "These values appear to be random floats. However, now that the word has been transformed into a vector, we can more easily perform computations on it. \n",
    "\n",
    "Let's take a look at a few examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f917cd-f319-425d-8452-baa8c0598b82",
   "metadata": {},
   "source": [
    "# 2. Word Similarity and Analogy\n",
    "\n",
    "Following the example phrase above about word embedding for \"bank, the first question we can ask is: what words are similar to \"bank\"? In vector space, we'd expect similar words to have vectors that are closer to each other.\n",
    "\n",
    "There are many metrics for measuring vector similarity, one of the most useful being [**cosine similarity**](https://en.wikipedia.org/wiki/Cosine_similarity). Cosine similarity ranges from 0 to 1, with orthogonal vectors having a cosine similarity of 0 and parallel vectors having a cosine similarity of 1.\n",
    "\n",
    "`gensim` provides a function called `most_similar()` that lets us find the words most similar to a queried word. The output is a tuple of the word and its cosine similarity to the queried word.\n",
    "\n",
    "Let's give it a shot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f6340f-cb0d-49f1-af0e-c584400f278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar(['bank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133dd9f2-115e-4b75-9379-4f465e117b97",
   "metadata": {},
   "source": [
    "It looks like most similar vectors to \"bank\" are other financial terms! \n",
    "\n",
    "Recall that `word2vec` is trained to capture a word's meaning based on contextual information. These results pop up because these words commonly appear in similar contexts as the word \"bank\". \n",
    "\n",
    "In addition to querying the most similar words, we can also ask the model to return the cosine similarity between two words by calling the function `similarity()`.\n",
    "\n",
    "Let's go ahead and check out the similarities between the following pairs of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408f35ab-60f9-4d23-a4bf-9ddc75a7e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# banana\n",
    "wv.similarity('bank', 'banana')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee556605-b553-4ec3-b761-b83d737dce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# river \n",
    "wv.similarity('bank', 'river')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cddb8ed-d984-4c2f-9b48-2f8545ce44a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bank with a capitalized B\n",
    "wv.similarity('Bank', 'river')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd07f40-81bf-4aa8-826d-a06f723deae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the present participle of bank\n",
    "wv.similarity('banking', 'river')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217561a7-8dfe-4537-839d-0fa6b9063d9e",
   "metadata": {},
   "source": [
    "**Question**: Why do \"bank\" and \"river\" appear to have higher similarity than other pairs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a73249a-b6c6-4a41-82c3-4fbc0947c26f",
   "metadata": {},
   "source": [
    "## Word Analogy\n",
    "\n",
    "One of the most famous usages of `word2vec` is via word analogies. For example:\n",
    "\n",
    "`man : king :: woman : queen`\n",
    "\n",
    "Oftentimes, word analogy like this is visualized with parallelogram, such as shown in the following figure, which is adapted from [Ethayarajh et al. (2019)](https://aclanthology.org/P19-1315.pdf). \n",
    "\n",
    "<img src='Images/word_analogy.png' alt=\"Word analogy\" width=\"450\">\n",
    "\n",
    "The upper side (difference between `man` and `woman`) should approximate the lower side (difference between `king` and `queen`); the vector difference represents the meanig of `female`. \n",
    "\n",
    "- $\\mathbf{V}_{\\text{man}} - \\mathbf{V}_{\\text{woman}} \\approx \\mathbf{V}_{\\text{king}} - \\mathbf{V}_{\\text{queen}}$\n",
    "\n",
    "Similarly, the left side (difference between `king` and `man`) should approximate the right side (difference between `queen` and `woman`); the vector difference represents the meaning of `royal`.\n",
    "\n",
    "- $\\mathbf{V}_{\\text{king}} - \\mathbf{V}_{\\text{man}} \\approx \\mathbf{V}_{\\text{queen}} - \\mathbf{V}_{\\text{woman}}$\n",
    "\n",
    "We can take either equation and rearrange it:\n",
    "\n",
    "- $\\mathbf{V}_{\\text{king}} - \\mathbf{V}_{\\text{man}} + \\mathbf{V}_{\\text{woman}} \\approx \\mathbf{V}_{\\text{Queen}}$\n",
    "\n",
    "If the vectors of `king`, `man`, and `woman` are known, by vector arithmatics we should be able to get a vector that approximates the meaning of `queen`. \n",
    "\n",
    "Let's implement it using the word2vec model and the `get_vector` function. Note: In all the operations below, we set `norm=True`, and renormalize. That's because different vectors might be of different lengths, so the normalization puts everything on a common scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362bba64-01b0-4c1c-8e19-43365dd0834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate \"royal\" vector difference\n",
    "difference = wv.get_vector('king', norm=True) - wv.get_vector('man', norm=True) \n",
    "\n",
    "# Add on woman\n",
    "difference += wv.get_vector('woman', norm=True)\n",
    "\n",
    "# Renormalize vector\n",
    "difference = difference / np.linalg.norm(difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d25a4d-5012-4035-8833-ce7f15afbed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the most similar vector?\n",
    "wv.most_similar(difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680c755c-f22c-4e77-b8c2-c65a2f5a95a6",
   "metadata": {},
   "source": [
    "The word \"queen\" is the second most similar one. \n",
    "\n",
    "Carrying out these operations can be done in one swoop with the `most_similar` function. \n",
    "\n",
    "We pass in two arguments `positive` and `negative`, wherein `positive` holds the words that we want the output to be similar with, and `negative` the words we'd like the output to be dissimilar with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce536bb-fdd7-4afb-98be-3b30f92770e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar(positive=['woman', 'king'], negative='man')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85148bd-8d5c-4898-aceb-3185d132dcca",
   "metadata": {},
   "source": [
    "# 3. Semantic Axes and Associations in Word Embeddings\n",
    "\n",
    "### `glove` \n",
    "\n",
    "As you can work through in the practice code problems, we can show that gender bias is present in the pre-trained word embeddings. Let's take a closer look at it!\n",
    "\n",
    "We will switch from `word2vec` to a smaller size embedding, the pretrained `glove`, starting from this section. Let's load it with the `api.load()` function. \n",
    "\n",
    "The model we load in is trained from Wikipedia and Gigaword (news data). Check out the [documentation](https://nlp.stanford.edu/projects/glove/) if you want to know further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4676dca5-0be0-43dd-91ec-4ffe03928743",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = api.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e298be4-6521-4af7-82eb-e3a607ea5938",
   "metadata": {},
   "source": [
    "Let's double check the size of the embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881a897d-1215-4e58-847e-6c6245ad6aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove['banana'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c01777c-baa9-4122-81da-9626e0c730df",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv['banana'].size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56330365-6bcd-4fd5-b28a-0eb34ee66634",
   "metadata": {},
   "source": [
    "### Semantic Axis\n",
    "\n",
    "To investigate gender bias in word embeddings. We first need a vector representation that captures the concept of gender. The idea is to construct **a semantic axis** (or \"SemAxis\") of the concept. This concept is often complex, and cannot be simply denoted by a single word. And it is often fluid, meaning that its meaning spans from one end to the other. Once we've got the vector representation of this concept, we can project a list of terms onto that axis, and see if each of the terms is more aligned towards one end or the other of the concept. \n",
    "\n",
    "The methods of doing so come from [An et al. 2018](https://aclanthology.org/P18-1228/). We will first need to come up with two lists of pole words, which are opposing to each other. \n",
    "\n",
    "- $\\mathbf{V}_{\\text{plus}} = \\{v_{1}^{+}, v_{2}^{+}, v_{3}^{+}, ..., v_{n}^{+}\\}$\n",
    "\n",
    "- $\\mathbf{V}_{\\text{minus}} = \\{v_{1}^{-}, v_{2}^{-}, v_{3}^{-}, ..., v_{n}^{-}\\}$\n",
    "\n",
    "We take the mean of each vector set to represent the core meaning of that set. \n",
    "\n",
    "- $\\mathbf{V}_{\\text{plus}} = \\frac{1}{n}\\sum_{1}^{n}v_{i}^{+}$\n",
    "\n",
    "- $\\mathbf{V}_{\\text{minus}} = \\frac{1}{n}\\sum_{1}^{n}v_{j}^{-}$\n",
    "\n",
    "Next we take the difference between the two means to represent the corresponding semantic axis. \n",
    "\n",
    "- $\\mathbf{V}_{\\text{axis}} = \\mathbf{V}_{\\text{plus}} - \\mathbf{V}_{\\text{minus}}$\n",
    "\n",
    "Projecting a specific term to the semantic axis is, as we've learned above, operationalized as taking the `cosine similarity` between the word's vector and the semantic axis vector. A positive value would indicate that the term is more closer to the $\\mathbf{V}_{\\text{plus}}$ end, and a negative value meaning proximity to the $\\mathbf{V}_{\\text{minus}}$ end. \n",
    "\n",
    "- $score(w) = cos(v_{w},  \\mathbf{V}_{\\text{axis}})$\n",
    "\n",
    "*Warning:* A binary distinction of gender is a simplification of the diversity and complexity of gender identities. This method is limited, as it is only capable of constructing two polarities. Along the way, we'll discover how much stereotyping is encoded in it.\n",
    "\n",
    "We will contruct the gender semantic axis using two sets of pole words for \"female\" and \"male\" from Bolukbasi et al. (2016). We will get the embeddings for these words from `glove` to calculate the gender axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094d3a8e-f605-4954-a6af-dc4d16d58b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two sets of pole words (examples from Bolukbasi et al., 2016)\n",
    "female = ['she', 'woman', 'female', 'daughter', 'mother', 'girl']\n",
    "male = ['he', 'man', 'male', 'son', 'father', 'boy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e4d99b-0da1-4177-aa47-0db2e7a53a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semaxis(list1, list2, model, embedding_size):\n",
    "    '''Calculate the embedding of a semantic axis given two lists of pole words.'''\n",
    "\n",
    "    # STEP 1: Get the embeddings for terms in each list\n",
    "    vplus = [model[term] for term in list1]\n",
    "    vminus = [model[term] for term in list2]\n",
    "\n",
    "    # Step 2: Calculate the mean embeddings for each list\n",
    "    vplus_mean = np.mean(vplus, axis=0)\n",
    "    vminus_mean = np.mean(vminus, axis=0)\n",
    "\n",
    "    # Step 3: Get the difference between two means\n",
    "    sem_axis = vplus_mean - vminus_mean\n",
    "\n",
    "    # Sanity check\n",
    "    assert sem_axis.size == embedding_size\n",
    "    \n",
    "    return sem_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88257610-b8e7-4429-8d1f-dabe3de70891",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plug in the gender lists to calculate the semantic axis for gender\n",
    "gender_axis = get_semaxis(list1=female, \n",
    "                          list2=male, \n",
    "                          model=glove, \n",
    "                          embedding_size=50)\n",
    "gender_axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748fa605-d07f-45ca-b786-795d034c6a61",
   "metadata": {},
   "source": [
    "We have the gender axis ready! The next step is to project a list of terms onto the gender axis. Let's test it with a set of occupation terms which might be associated with gender stereotypes. \n",
    "\n",
    "Before we go ahead to calculate the cosine similarity, let first rate the following occupation terms, use your intuition!\n",
    "\n",
    "The rating should be between $[-1, 1]$: the negative value means the term is closer to the male end and positive value to the female end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd47f71-7691-412d-be2e-046b0f13c800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of occupations terms (examples taken from Bolukbaski et al., 2016)\n",
    "occupations = ['engineer',\n",
    "               'nurse',\n",
    "               'designer',\n",
    "               'receptionist',\n",
    "               'banker',\n",
    "               'librarian',\n",
    "               'architect',\n",
    "               'hairdresser',\n",
    "               'philosopher']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baab8481-40a5-4e7a-8ff1-4c0b086b5e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rate the following occupation terms\n",
    "occ_rating = {'engineer': ...,\n",
    "              'nurse': ...,\n",
    "              'designer': ...,\n",
    "              'receptionist': ...,\n",
    "              'banker': ...,\n",
    "              'librarian': ...,\n",
    "              'architect': ...,\n",
    "              'hairdresser': ...,\n",
    "              'philosopher': ...\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9fed84-60fd-4d4b-9a07-658484c30aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between a given word and the axis\n",
    "def get_projection(word, model, axis):\n",
    "    '''Get the projection of a word onto a semantic axis'''\n",
    "    \n",
    "    word_norm = model[word] / np.linalg.norm(model[word])\n",
    "    axis_norm = axis / np.linalg.norm(axis)\n",
    "    projection = np.dot(word_norm, axis_norm) \n",
    "    \n",
    "    return projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e537b5-165a-41be-8705-2f3d0092e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_projections = {word: get_projection(word, glove, gender_axis) for word in occupations}\n",
    "occ_projections "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c967ce2-1eac-483b-b46e-056bb0b8a8a9",
   "metadata": {},
   "source": [
    "## Visualize the Projection\n",
    "\n",
    "Now that we have calculated the projection of each occupation term onto the gender axis, let's plot these values to gain a more straightforward understanding of how much gender stereotyping is hidden in these terms.\n",
    "\n",
    "We will use a bar plot to visualize them, with the color of each bar corresponding to the proximity of a term to an end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0394ceb7-b50f-4d70-9443-0e255b4c4074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import Normalize\n",
    "\n",
    "def plot_semantic_axis(projections, title, xlab):\n",
    "    '''Return a horizontal bar plot of the projections.'''\n",
    "\n",
    "    # Sort the projections in descending order\n",
    "    projection_sorted = sorted(projections.items(), key=lambda term: term[1], reverse=True)\n",
    "\n",
    "    # Extract the terms\n",
    "    terms = [term_value[0] for term_value in projection_sorted]\n",
    "\n",
    "    # Extract corresponding values of projections\n",
    "    values = [term_value[1] for term_value in projection_sorted]\n",
    "\n",
    "    # Take the absolute values for gradient color fill\n",
    "    values_abs = np.abs(values)\n",
    "    norm = Normalize(vmin=min(values_abs), vmax=max(values_abs))\n",
    "    cmap = plt.get_cmap(\"YlOrBr\")  \n",
    "    colors = [cmap(norm(value)) for value in values_abs]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))  \n",
    "    plt.barh(terms, values, color=colors)\n",
    "    plt.grid(axis=\"x\", linestyle=\":\", alpha=0.5)\n",
    "    plt.xlim(-np.max(values_abs+0.05), np.max(values_abs+0.05))\n",
    "    plt.xlabel(xlab)\n",
    "    plt.title(title)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fd4b50-e565-4e50-b063-7b88fba858e0",
   "metadata": {},
   "source": [
    "We will visualize the projections as well as your self-ratings together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1286e06-9cf5-4bed-a755-987fbd53c239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "title1 = 'Projections onto the gender axis'\n",
    "title2 = 'Self-rated projections onto the gender axis'\n",
    "xlab = 'Gender-stereotypical occpuation terms'\n",
    "\n",
    "plot_semantic_axis(occ_projections, title1, xlab)\n",
    "plot_semantic_axis(occ_rating, title2, xlab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1490b4ca-41a9-4a0e-99d1-fb247f3927ff",
   "metadata": {},
   "source": [
    "**Question**: Do you find the results surprising or expected? Why does gender stereotyping exist in word embeddings?\n",
    "\n",
    "## The Class Axis\n",
    "\n",
    "In addition to projecting terms onto a single axis, we can also project terms onto two axes and plot the results on a scatter plot, where the coordinates correspond to projections onto the two axes.\n",
    "\n",
    "Social class is another dimension that has been frequently discussed in the literature. In this example, we'll create a semantic axis for social class, using two sets of pole words representing the two ends of class, as described in [Kozlowski et al. 2019](https://journals.sagepub.com/doi/full/10.1177/0003122419877135).\n",
    "\n",
    "First, we'll project a list of sports terms onto both the gender and social class axes, similar to the method used in [Kozlowski et al. 2019](https://journals.sagepub.com/doi/full/10.1177/0003122419877135). We'll visualize the results on a scatter plot, with the x-axis representing gender and the y-axis representing social class. The coordinates of a term on this plot correspond to its projections onto these axes.\n",
    "\n",
    "Next, we'll repeat the process to visualize occupation terms, which will give us a rough idea of how much a term is biased towards either end of these two dimensions.\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3597678-99ef-45b3-a78b-26b529db0acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two sets of pole words of social class (examples taken from Kozlowski et. al, 2019)\n",
    "poor = ['poor', 'poorer', 'poorest', 'poverty', 'inexpensive', 'impoverished', 'cheap']\n",
    "rich = ['rich', 'richer', 'richest', 'affluence', 'expensive', 'wealthy', 'luxury']\n",
    "\n",
    "class_axis = get_semaxis(list1=poor, \n",
    "                         list2=rich, \n",
    "                         model=glove,\n",
    "                         embedding_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a1ad74-c6a4-46ac-a7ce-78090d283f76",
   "metadata": {},
   "source": [
    "We will project sports terms onto the social class axis to see if some sports are more associated with \"high\" society and others with \"low\" society. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b5d449-cca7-435a-9a33-65a622dcb340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of sports terms (examples taken from Kozlowski et. al, 2019)\n",
    "sports = ['camping', \n",
    "          'boxing', \n",
    "          'bowling', \n",
    "          'baseball', \n",
    "          'soccer', \n",
    "          'tennis', \n",
    "          'golf', \n",
    "          'basketball', \n",
    "          'skiing', \n",
    "          'sailing', \n",
    "          'volleyball']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56c3592-9760-4eb6-acda-bdc931c6284f",
   "metadata": {},
   "source": [
    "Next, let's use the `get_projection` function to calculate the cosine similarity between each sport term and the axis (gender and class). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e125157-e645-477a-8c8d-52932a68dba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_spt_class = {word: get_projection(word, glove, class_axis) for word in sports}\n",
    "proj_spt_gender = {word: get_projection(word, glove, gender_axis) for word in sports}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1ec3d1-609a-4d2f-b9b3-25131a536e89",
   "metadata": {},
   "source": [
    "Finally, let's plot the results in a scatter plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a538e1-809f-49e5-94d3-c46888761229",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 7))\n",
    "\n",
    "# Use scatter plot to visualize the results\n",
    "plt.scatter(list(proj_spt_gender.values()), \n",
    "            list(proj_spt_class.values()), \n",
    "            color='cornflowerblue',\n",
    "            s=75)\n",
    "\n",
    "# Add text label to each dot\n",
    "for term in sports:\n",
    "    plt.annotate(term, \n",
    "                 (proj_spt_gender[term], proj_spt_class[term]), \n",
    "                 fontsize=10)\n",
    "\n",
    "# Add more annotations to four corners of the plot\n",
    "plt.annotate('Male/High', (-0.48, -0.28), color='gray', horizontalalignment='left')\n",
    "plt.annotate('Female/High', (0.48, -0.28), color='gray', horizontalalignment='right')\n",
    "plt.annotate('Male/Low', (-0.48, 0.27), color='gray', horizontalalignment='left')\n",
    "plt.annotate('Female/Low', (0.48, 0.27), color='gray', horizontalalignment='right')\n",
    "\n",
    "# Add reference lines to each semantic axis\n",
    "plt.hlines(xmin=-1, xmax=1, y=0, color='lightcoral', linewidth=1, linestyle=':')\n",
    "plt.vlines(ymin=-1, ymax=1, x=0, color='lightcoral', linewidth=1, linestyle=':')\n",
    "\n",
    "# Other parameter settings\n",
    "plt.xlim(-0.5, 0.5)\n",
    "plt.ylim(-0.3, 0.3)\n",
    "plt.grid(True, linestyle=':')\n",
    "plt.xlabel('Projection onto Gender')\n",
    "plt.ylabel('Projection onto Class')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b30b78-dd17-4bd5-9028-29e3f37cb963",
   "metadata": {},
   "source": [
    "**Questions**: \n",
    "- Which sport term is most biased towards male and which toward female?\n",
    "- Which sport seems to be gender-neutral?\n",
    "- Which sport term is most biased towards high social class, and which towards low social class?\n",
    "- Which sport seems to be neutral to class?\n",
    "\n",
    "Ok! Let's go back to occupation terms. We will first add a few new occupations to the list, and then get the projections onto both axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaee050-9932-4aeb-a4db-87d46aeccac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations = ['engineer',\n",
    "               'nurse',\n",
    "               'designer',\n",
    "               'receptionist',\n",
    "               'banker',\n",
    "               'librarian',\n",
    "               'architect',\n",
    "               'hairdresser',\n",
    "               'philosopher',\n",
    "               'plumber',\n",
    "               'police',\n",
    "               'pilot',\n",
    "               'cashier',\n",
    "               'janitor']\n",
    "\n",
    "proj_occ_gender = {word: get_projection(word, glove, gender_axis) for word in occupations}\n",
    "proj_occ_class = {word: get_projection(word, glove, class_axis) for word in occupations}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793cd949-b91e-498a-8b76-ce2368829f81",
   "metadata": {},
   "source": [
    "Next, let's visualize the results in a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66140280-3d5d-4aa9-9b76-781399f2c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 7))\n",
    "\n",
    "# Use scatter plot to visualize the results\n",
    "plt.scatter(list(proj_occ_gender.values()), \n",
    "            list(proj_occ_class.values()), \n",
    "            color='tan', \n",
    "            s=75)\n",
    "\n",
    "# Add text label to each dot\n",
    "for term in occupations:\n",
    "    plt.annotate(term, \n",
    "                 (proj_occ_gender[term], proj_occ_class[term]), \n",
    "                 fontsize=10)\n",
    "\n",
    "# Add more annotations to four corners of the plot\n",
    "plt.annotate('Male/High', (-0.48, -0.48), color='gray', horizontalalignment='left')\n",
    "plt.annotate('Female/High', (0.48, -0.48), color='gray', horizontalalignment='right')\n",
    "plt.annotate('Male/Low', (-0.48, 0.45), color='gray', horizontalalignment='left')\n",
    "plt.annotate('Female/Low', (0.48, 0.45), color='gray', horizontalalignment='right')\n",
    "\n",
    "# Add reference lines to each semantic axis\n",
    "plt.hlines(xmin=-1, xmax=1, y=0, color='lightcoral', linewidth=1, linestyle=':')\n",
    "plt.vlines(ymin=-1, ymax=1, x=0, color='lightcoral', linewidth=1, linestyle=':')\n",
    "\n",
    "# Other parameter settings\n",
    "plt.xlim(-0.5, 0.5)\n",
    "plt.ylim(-0.5, 0.5)\n",
    "plt.grid(True, linestyle=':')\n",
    "plt.xlabel('Projection onto Gender')\n",
    "plt.ylabel('Projection onto Class')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93b7bf-f88e-460d-bcf7-f760597da95c",
   "metadata": {},
   "source": [
    "**Questions**: \n",
    "- Which occupation is most biased towards high social class, and which towards low social class?\n",
    "- Which occupation seems to be neutral to class?\n",
    "- Are any of the social class mappings surprising?\n",
    "- Are any of the gender mappings surprising?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ec74c2-44d0-4061-888c-bb1d7e9e436a",
   "metadata": {},
   "source": [
    "Hopefully these plots leave you with some food for thought to further explore word embeddings. Constructing an axis of gender or social class has been widely researched, but with the tool of semantic axis, we can investigate much more. It is useful for capturing the abstract meaning of various notions, such as an axis of coldness, an axis of kindness, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc2acd5-fe77-4bee-916c-94d2443d3082",
   "metadata": {},
   "source": [
    "## Key Points\n",
    "\n",
    "* Pre-trained word embeddings like `word2vec` and `glove` take contextual information into representations of words' meanings. \n",
    "* Similarities between words is conveniently reflected in cosine similarity. \n",
    "* We can explore biases in word embeddings with the methods of semantic axis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8cf263-e025-43d5-90aa-4323bcaf6e24",
   "metadata": {},
   "source": [
    "# 4. Bringing it together: Predicting tweet sentiment\n",
    "\n",
    "In the previous notebook, we used machine learning with vocabulary as features to predict pre-classified tweet sentiment. But what if we did not have access to pre-classified tweet sentiment and needed to define it ourselves based on the data? \n",
    "\n",
    "In this final text analysis activity, we will use the concept of semantic axes to create a vector representation of positive vs negative sentiment. We will then apply this axis to the vocabulary in the airline tweets dataset, and use this as a basis for determining sentiment.\n",
    "\n",
    "The first thing we will do is load and prep the tweet data following the steps in notebook 9b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d997beb3-ab6a-4cd5-976f-ca15370bf727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clean dataset in\n",
    "tweets_path = 'Data/tweets_clean.csv'\n",
    "tweets = pd.read_csv(tweets_path, sep=',')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd06031-4064-4dd9-a377-1b5b0f475173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function to create a document-term matrix (DTM)\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186c4624-44d5-42ec-9e94-f34397425d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize the parameter setting\n",
    "vectorizer = CountVectorizer(lowercase=True,\n",
    "                             stop_words='english',\n",
    "                             min_df=2,\n",
    "                             max_df=0.95,\n",
    "                             max_features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee841bed-1233-4236-abf2-77fcdb6c8d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit, transform, and get tokens\n",
    "counts = vectorizer.fit_transform(tweets['text_lemmatized'])\n",
    "tokens = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create the second DTM\n",
    "dtm = pd.DataFrame(data=counts.todense(),\n",
    "                          index=tweets.index,\n",
    "                          columns=tokens)\n",
    "print(dtm.shape)\n",
    "# get most frequent tokens\n",
    "dtm.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466c8765-b71c-4e6e-ac34-5327f1a8c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0e60d0-0597-48cc-aa10-7c9ab26354c5",
   "metadata": {},
   "source": [
    "Now we prepare the semantic axis using terms capturing positivity and negativity. I use the words from a list in [An et al 2018](https://yongyeol.com/papers/an2018semaxis.pdf) that conducted a similar exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b506e8-c911-4d97-a01c-e10c4801994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two sets of pole words (examples from An et al., 2018)\n",
    "positive = ['good', 'lovely', 'excellent', 'fortunate', 'pleasant', 'delightful', 'perfect', 'loved', 'love', 'happy',\n",
    "           'awesome', 'nice', 'amazing', 'best', 'fantastic']\n",
    "negative = ['bad', 'horrible', 'poor', 'unfortunate', 'unpleasant', 'disgusting', 'evil', 'hated', 'hate', 'unhappy',\n",
    "           'terrible', 'nasty', 'awful', 'worst', 'sad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2771975-80be-408a-8ae6-c2080a29af0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plug in the positive/negative lists to calculate the semantic axis for positive sentiment\n",
    "# Using the same function we created above\n",
    "positive_axis = get_semaxis(list1=positive, \n",
    "                          list2=negative, \n",
    "                          model=glove, \n",
    "                          embedding_size=50)\n",
    "positive_axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92515772-a8c9-4d14-8386-b9e8958e91d6",
   "metadata": {},
   "source": [
    "Ok, now we have a document-term matrix indicating which terms from the full corpus appear in each tweet/document, and a semantic axis against which to evaluate these terms. We will first compute positivity scores for each term in the DTM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0113a8-a0a1-4104-b1f4-3fd8af223ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cosine_similarity function from scikit-learn\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff1a248-81c9-4cdf-84a4-981228ce0f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute positivity score for each word\n",
    "word_scores = {}\n",
    "for word in tokens:\n",
    "    if word in glove:\n",
    "        word_vector = glove[word].reshape(1, -1)\n",
    "        score = cosine_similarity(word_vector, positive_axis.reshape(1, -1))[0, 0]  # Cosine similarity\n",
    "        word_scores[word] = score\n",
    "    else:\n",
    "        word_scores[word] = 0  # Assign zero if word is missing from embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8e53c-5883-40a6-9fd3-4acbe03d6967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert word_scores into a DataFrame\n",
    "word_scores_df = pd.DataFrame(list(word_scores.items()), columns=[\"word\", \"positivity_score\"])\n",
    "# Let's browse some of these word scores. How do they look?\n",
    "word_scores_df[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4bdf17-ef4e-4371-a65f-c4c1d314eb4b",
   "metadata": {},
   "source": [
    "We will now use those scores and the DTM to compute overall positivity for each tweet. Formaly, we will use matrix-vector multiplication to perform a dot product which sums up word positivity scores for each tweet. We will then normalize these scores across all tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fed132-3286-4f4d-a7c4-2d76cc38a06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute positivity for each tweet\n",
    "tweets['positive_score'] = dtm.dot(pd.Series(word_scores))  # Multiply DTM matrix by word scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f543ee33-c1da-41a6-9181-a4566aaef9fa",
   "metadata": {},
   "source": [
    "We've done it! Let's see how these estimated positivity scores line up against the labels provided in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0780087a-de33-49ac-882e-6050eba4ac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot distributions for positive and negative tweets\n",
    "sns.kdeplot(tweets.loc[tweets['airline_sentiment'] == 'positive', 'positive_score'], \n",
    "            label=\"Positive Sentiment\", fill=True, color='green', alpha=0.5)\n",
    "\n",
    "sns.kdeplot(tweets.loc[tweets['airline_sentiment'] == 'negative', 'positive_score'], \n",
    "            label=\"Negative Sentiment\", fill=True, color='red', alpha=0.5)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Positive Score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of Positive Score by Sentiment\")\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d4cd61-ca7f-448f-8d55-7698dbc7e19f",
   "metadata": {},
   "source": [
    "There's quite a bit of overlap here. Is there any signal in the positivity score we created?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15a0389-f6d6-489e-842d-9722618e075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Compute mean positive scores by airline_sentiment\n",
    "mean_scores = tweets.groupby(\"airline_sentiment\")[\"positive_score\"].mean().reset_index()\n",
    "\n",
    "# Perform an independent t-test (assuming only \"positive\" and \"negative\" labels exist)\n",
    "pos_scores = tweets.loc[tweets['airline_sentiment'] == 'positive', 'positive_score']\n",
    "neg_scores = tweets.loc[tweets['airline_sentiment'] == 'negative', 'positive_score']\n",
    "t_stat, p_value = ttest_ind(pos_scores, neg_scores, equal_var=False)  # Welchâ€™s t-test\n",
    "\n",
    "# Create a barplot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(data=mean_scores, x=\"airline_sentiment\", y=\"positive_score\", \n",
    "            hue=\"airline_sentiment\", palette={\"positive\": \"green\", \"negative\": \"red\"}, legend=False)\n",
    "\n",
    "# Annotate with p-value\n",
    "plt.text(0.5, mean_scores[\"positive_score\"].max(), f'p-value = {p_value:.4f}', \n",
    "         ha='center', fontsize=12, color='black', fontweight='bold')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Airline Sentiment\")\n",
    "plt.ylabel(\"Mean Positive Score\")\n",
    "plt.title(\"Mean Positive Score by Sentiment Category\")\n",
    "plt.ylim(0, mean_scores[\"positive_score\"].max() + 0.05)  # Adjust y-axis for clarity\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef54e653-1818-40e6-a4f7-df3ce1aa75ae",
   "metadata": {},
   "source": [
    "Let's see if we can get a more specific measure of sentiment by only calculating tweet-level sentiment using words with larger absolute values on the positivity semantic axes. Many words many have low positive values but not really help with identifying sentiment. Setting those aside may make the distinction more clear. Let's first look at the distribution of word positivity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1b0b95-d79e-4bf4-914d-5004029aaabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot distributions of word-level positivity scores\n",
    "sns.kdeplot(word_scores_df['positivity_score'], \n",
    "            label=\"Positive Sentiment\", fill=True, color='green', alpha=0.5)\n",
    "plt.xlabel(\"Positive Score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of Positive Score by Word\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac1a482-1ea3-4769-a34d-a8af62b6926b",
   "metadata": {},
   "source": [
    "Let's set a threshold of -0.1 for negative words and 0.2 for positive words, and set everything else equal to 0. Then we'll recompute the tweet-level positivity scores, and see how it lines up with the sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8435c97d-9ca7-4c5e-bc6a-8f6b093285db",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_scores2 = {word: (0 if -0.1 <= score <= 0.2 else score) for word, score in word_scores.items()}\n",
    "tweets['positive_score2'] = dtm.dot(pd.Series(word_scores2))  # Multiply DTM matrix by word scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0144c281-30ee-4281-b2ca-d2879017f2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot distributions for positive and negative tweets\n",
    "sns.kdeplot(tweets.loc[tweets['airline_sentiment'] == 'positive', 'positive_score2'], \n",
    "            label=\"Positive Sentiment\", fill=True, color='green', alpha=0.5)\n",
    "\n",
    "sns.kdeplot(tweets.loc[tweets['airline_sentiment'] == 'negative', 'positive_score2'], \n",
    "            label=\"Negative Sentiment\", fill=True, color='red', alpha=0.5)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Positive Score, Revised\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of Positive Score by Sentiment\")\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704b213b-f47e-4f81-ac11-a1a74cbb1515",
   "metadata": {},
   "source": [
    "**Question**: How does it look? How does it compare to the initial graph?\n",
    "\n",
    "The original dataset included a measure of certainty in their sentiment classification. Let's use this to create a continuous measure of positivity, and plot this against our positivity measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754af46e-38f3-4c31-9337-63ee099ee379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'sentiment_continuous' variable\n",
    "tweets['sentiment_continuous'] = np.where(\n",
    "    tweets['airline_sentiment'] == 'positive', tweets['airline_sentiment_confidence'],\n",
    "    np.where(tweets['airline_sentiment'] == 'negative', -tweets['airline_sentiment_confidence'], 0)\n",
    ")\n",
    "tweets['sentiment_continuous'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49626d6b-24c8-4b71-863f-887008ecf885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Perform linear regression\n",
    "X = tweets['positive_score2']\n",
    "y = tweets['sentiment_continuous']\n",
    "X = sm.add_constant(X)  # Add intercept for regression\n",
    "model = sm.OLS(y, X).fit()  # Fit regression model\n",
    "r_squared = model.rsquared  # Extract RÂ² value\n",
    "\n",
    "# Plot scatterplot with regression line\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.regplot(x=tweets['positive_score2'], y=tweets['sentiment_continuous'], scatter_kws={'alpha': 0.5}, line_kws={\"color\": \"red\"})\n",
    "\n",
    "# Annotate RÂ² value on the plot\n",
    "plt.text(0.05, 0.8, f'RÂ² = {r_squared:.4f}', fontsize=12, color='black', fontweight='bold', transform=plt.gca().transAxes)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Positive Score, Revised\")\n",
    "plt.ylabel(\"Sentiment Continuous\")\n",
    "plt.title(\"Relationship between Positive Score and Sentiment Confidence\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0af161-b4d9-48cb-a28e-0551799e3cab",
   "metadata": {},
   "source": [
    "**Question:** What do we conclude? What more could we try to do?\n",
    "\n",
    "**Challenge:** See if you can improve on this process to create a measure of sentiment that better matches the labels provided in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51013195-b691-4234-8446-de6d8194b5f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
