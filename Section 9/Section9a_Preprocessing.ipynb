{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3e7ea21-6437-48e8-a9e4-3bdc05f709c9",
   "metadata": {},
   "source": [
    "# Section 9. Text Analysis Preprocessing\n",
    "\n",
    "#### Instructor: Pierre Biscaye \n",
    "\n",
    "This is the first of three notebooks covering the foundations for performing **text analysis** in Python. These techniques lie in the domain of Natural Language Processing (NLP). NLP is a field that deals with identifying and extracting patterns of language, primarily in written texts. We will cover a variety of basic text analysis tasks using both built-in python string methods as well as specific NLP packages, such as `nltk`, `spaCy`, and more recent ones on Large Language Models (`transformers`).\n",
    "\n",
    "The content of this notebook is taken from UC Berkeley D-Lab's Python Text Analysis [course](https://github.com/dlab-berkeley/Python-Text-Analysis).\n",
    "\n",
    "### Sections\n",
    "1. Preprocessing: Apply common steps for preprocessing text data, applied to the cae of Twitter data.\n",
    "2. Tokenization: Understand tokenizers and differences using `nltk` and `spaCy`, and how they have changed since the advent of Large Language Models using `BERT`. \n",
    "\n",
    "Before starting, make sure you have the below packages properly installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d442e4c7-e926-493d-a64e-516616ad915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install the following packages\n",
    "# %pip install NLTK\n",
    "# %pip install transformers\n",
    "# %pip install spaCy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5b8f8e-4e69-426e-a202-ec48b325e89a",
   "metadata": {},
   "source": [
    "# 1. Preprocessing\n",
    "\n",
    "The first step of text analysis is to convert the raw, messy text data into a consistent format. Doing so ensures that the text data preserves the necessary information for subsequent analysis while stripping away less relevant details. This process is often called **preprocessing**, **text cleaning**, or **text normalization**.\n",
    "\n",
    "You'll notice that at the end of preprocessing, our data is still in a format that we can read and understand - it remains 'text.' In Parts 2 and 3, we will begin our foray into converting the text data into numerical representations — formats that can be more readily handled by computers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b35911a-3b3f-4a48-a7d1-9882aab04851",
   "metadata": {},
   "source": [
    "## Common Processes\n",
    "\n",
    "Preprocessing is not something we can accomplish in one swoop with a single line of code. We often start by familiarizing ourselves with the data, and along the ways we become clearer about the granularity of preprocessing we need to do. We must also think about our analysis objectives, as this will guide the processing.\n",
    "\n",
    "Regardless of the objective, we typically begin with a set of commonly-used processes to clean the data. These operations will not substantially alter the form or meaning of the data; instead, they help us to convert the text data into a standard format. \n",
    "\n",
    "The following processes, for example, are commonly applied to preprocess texts of various genres. Many of these operations can be done with built-in Python functions, such as `string` methods and Regular Expressions. \n",
    "- *Lowercase* the text\n",
    "- Remove *punctuation* marks\n",
    "- Remove extra *whitespace* characters\n",
    "- Remove *stop words* (these will be specific to each language)\n",
    "- *Stemming/lemmatization* (also language-specific)\n",
    "\n",
    "Afterwards, we may choose to perform task-specific cleaning processes, the specifics of which are dependent on the task we want to perform later and the source from which we retrieve our data at the first place. \n",
    "\n",
    "Note that many of these tasks can/should be done after tokenization. The order of operations for preprocessing can have significant effects on what your cleaned text data look like.\n",
    "\n",
    "Before we jump into these operations, let's take a look at our data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d7350-9a1e-4db9-b828-a87fe1676d8d",
   "metadata": {},
   "source": [
    "### Import the Text Data\n",
    "\n",
    "The text data we'll be working with is contained in a CSV file. It contains tweets about U.S. airlines, scraped from February 2015. \n",
    "\n",
    "Let's read in the file `airline_tweets.csv` with `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1ff64b-53ad-4eca-b846-3fda20085c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the separator to be comma\n",
    "tweets = pd.read_csv('Data/airline_tweets.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e397ac6a-c2ba-4cce-8700-b36b38026c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first five rows\n",
    "print(tweets.shape)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3b339f-45cf-465d-931c-05f9096fd510",
   "metadata": {},
   "source": [
    "The dataframe has one row per tweet. The text of tweet is shown in the `text` column.\n",
    "- `text` (`str`): the text of the tweet.\n",
    "\n",
    "Other metadata we are interested in include: \n",
    "- `airline_sentiment` (`str`): the sentiment of the tweet, labeled as \"neutral\", \"positive\", or \"negative\". This is the result of previous natural language processing analysis.\n",
    "- `airline` (`str`): the airline that is tweeted about.\n",
    "- `retweet count` (`int`): how many times the tweet was retweeted.\n",
    "\n",
    "Let's take a look at some of the tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b690daab-7be5-4b8f-8af0-a91fdec4ec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets['text'].iloc[0])\n",
    "print(tweets['text'].iloc[1])\n",
    "print(tweets['text'].iloc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc05fa-ad30-4402-ab56-086bcb09a166",
   "metadata": {},
   "source": [
    "**Question**: What do you notice? What are some linguistic and/or stylistic features of Twitter data that might be relevant for preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3460393-00a6-461c-b02a-9e98f9b5d1af",
   "metadata": {},
   "source": [
    "### Lowercasing\n",
    "\n",
    "While we acknowledge that the **casing** of words can be informative, we often don't work in contexts where we can properly utilize this information. \n",
    "\n",
    "**Question:** What might be an example of when you would *want* to be able to use casing information for text analysis? \n",
    "\n",
    "More often, the analysis we perform is **case-insensitive**. For instance, in frequency analysis, we want to account for various forms of the same word. Lowercasing the text data aids in this process and simplifies our analysis.\n",
    "\n",
    "We can easily achieve text lowercasing with the built-in string function [`lower()`](https://docs.python.org/3/library/stdtypes.html#str.lower). See [string methods](https://www.w3schools.com/python/python_ref_string.asp) for more useful functions.\n",
    "\n",
    "Let's apply it to the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a95d90-3ef1-4bff-9cfe-d447ed99f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first example\n",
    "first_example = tweets['text'][108]\n",
    "print(first_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66d91c0-6eed-4591-95fc-cd2eae2e0d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the example is all lowercased\n",
    "print(first_example.islower())\n",
    "print(f\"{'=' * 50}\")\n",
    "\n",
    "# Convert it to lowercase\n",
    "print(first_example.lower())\n",
    "print(f\"{'=' * 50}\")\n",
    "\n",
    "# Convert it to uppercase\n",
    "print(first_example.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf0d8c8-bd6c-47ef-b305-09ac61d07d4d",
   "metadata": {},
   "source": [
    "### Remove Extra Whitespace Characters\n",
    "\n",
    "Sometimes we might come across texts with extraneous whitespace, such as space, tab, 'newline' characters, etc. This is particularly common when the text is scraped from web pages. Before we dive into the details, let's briefly introduce Regular Expressions (regexes) and the `re` package. \n",
    "\n",
    "Regular expressions are a powerful and efficient way of searching for specific string patterns in large corpora of text. Many NLP packages make heavy use of regexes under the hood. Regex testers are a useful tool in both understanding and creating regex expression. An example is [regex101](https://regex101.com).\n",
    "\n",
    "The goal in this notebook is not to provide a deep (or even shallow) dive into regexes. Instead, we want to expose you to them so that you are better prepared to do deep dives in the future!\n",
    "\n",
    "The following example is a poem by William Wordsworth. Like many other poems, the text may contain extra line breaks (or newline characters such as `\\n`) that we want to remove.\n",
    "\n",
    "Let's read the data in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bd73f1-a30f-4269-a05e-47cfff7b496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path to the poem\n",
    "text_path = 'Data/poem_wordsworth.txt'\n",
    "\n",
    "# Read the poem in\n",
    "with open(text_path, 'r') as file:\n",
    "    text = file.read()\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e78a75a-8e15-4bcb-a416-783aa7f60ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cce993-c315-4aaa-87fe-149de8607f65",
   "metadata": {},
   "source": [
    "As you can see, the poem is formatted as a continuous string of text with line breaks placed at the end of each line, which is difficult to read. \n",
    "\n",
    "One handy function we can use to display the poem properly is `splitlines()`. As the name suggests, it splits a long text sequence into a list of lines whenever there is a newline character.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeade7a-065d-49e6-bdd3-87a8ea8f6e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the single string into a list of lines\n",
    "text.splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3825b-0857-44e1-bf6a-d8c7a9032704",
   "metadata": {},
   "source": [
    "Let's return to our tweet data for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a81ea9-65c4-474a-8530-35393555d1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the second example\n",
    "tweets['text'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef55865-36fd-4c06-a765-530cf3b53096",
   "metadata": {},
   "source": [
    "In this case, we do not really want to split the tweet into a list of strings. We want to keep a single string of text but to remove the line break completely from the string.\n",
    "\n",
    "The string method `strip()` effectively does the job of stripping away spaces at either end of the text. However, it won't work in our example as the newline character is in the middle of the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b933503b-4370-4dc4-b287-6dc2f9cdb1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trip only removes blankspace at both ends\n",
    "tweets['text'][5].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99b80b4-804f-460f-a2d5-adbd654902b3",
   "metadata": {},
   "source": [
    "This is where regex could be really helpful.\n",
    "\n",
    "Let's load the package in first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceac9714-7053-4b2e-affb-71f8c3d2dcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import regex\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f08d20-ba81-4e48-9e2a-5728148005b3",
   "metadata": {},
   "source": [
    "Regex tools allow for both identifying portions of text matching a pattern you specify, and for doing some operations to text that matches the pattern. For example, you can extract it, replace it with something else, or remove it completely. The steps for substituting/replacing a given string pattern with an alternate string are as follows:\n",
    "\n",
    "- Define the target string pattern in regex (`r'PATTERN'`)\n",
    "- Define the replacement string for the pattern (`'REPLACEMENT'`)\n",
    "- Call the specific regex function to conduct this task/operation (in this case, `re.sub()`)\n",
    "\n",
    "In our example, the pattern we are looking for is `\\s`, which is the regex short name for any whitespace character (`\\n` and `\\t` included). We also add a quantifier `+` to the end: `\\s+`. It means we'd like to capture one or more occurences of the whitespace character. We will not get into more detail of how to use regex here - you can explore more on your own!\n",
    "\n",
    "The replacement for the alternative whitespace characters will be one single whitespace, which is the canonical word boundary in English. Any more whitespace will be reduced to one single whitespace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1248d227-1149-4014-94a5-c05592a27a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target pattern in regex\n",
    "blankspace_pattern = r'\\s+'\n",
    "\n",
    "# Define a replacement for the pattern identified\n",
    "blankspace_repl = ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12e3d1-728a-429b-9c83-4dcc88590bc4",
   "metadata": {},
   "source": [
    "Lastly, let's put everything together with the function [`re.sub()`](https://docs.python.org/3.11/library/re.html#re.sub), which means we want to substitute a pattern with a replacement. The function takes in three arguments—the pattern, the replacement, and the string to which we want to apply the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5249b24b-7111-4569-be29-c40efa5e148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace whitespace(s) with ' '\n",
    "clean_text = re.sub(pattern=blankspace_pattern, \n",
    "                    repl=blankspace_repl, \n",
    "                    string=tweets['text'][5])\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a895fbe3-a034-4124-94af-72a528913c51",
   "metadata": {},
   "source": [
    "Ta-da! The newline character is no longer there. \n",
    "\n",
    "### Extracting sub-strings\n",
    "\n",
    "We noticed that tweets often include Twitter handles, which are preceded by the character \"@\". We can use regular expressions to extract any handle tagged in a tweet! In particular, we can use `findall()` which extracts all strings meeting a predefined pattern. \n",
    "\n",
    "Let's test this with the first tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013664e8-36fc-4a46-b8aa-517117d78005",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets.loc[0,'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cfc70d-09d4-4cfc-995a-302a009a1f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.findall(r'@(\\S+)', tweets.loc[0,'text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b83668f-67f1-487d-876d-6e349cc394ba",
   "metadata": {},
   "source": [
    "Now let's apply this to all the tweets in the dataframe. We'll make a new column containing the twitter handles mentioned, separated by commas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db38ff9-56ff-4333-a63e-8d820dfd925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all mentions using regex\n",
    "tweets['mentions'] = tweets['text'].str.findall(r'@(\\S+)')\n",
    "\n",
    "# Convert list of mentions to a comma-separated string\n",
    "tweets['mentions'] = tweets['mentions'].apply(lambda x: ', '.join(x) if x else '')\n",
    "\n",
    "print(tweets.loc[0,'mentions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7087dc0c-5fef-4f1c-8662-7cbc8a978f34",
   "metadata": {},
   "source": [
    "### Remove Punctuation Marks\n",
    "\n",
    "Sometimes we are only interested in analyzing **alphanumeric characters** (i.e., the letters and numbers), in which case we might want to remove punctuation marks. This process becomes less common when we consider more advanced NLP algorithms.\n",
    "\n",
    "The `string` module contains a list of predefined punctuation marks. Let's print them out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e8502b-b703-45e0-8852-0c3210363440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in a predefined list of punctuation marks\n",
    "from string import punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91119c9e-431c-42cb-afea-f7e607698929",
   "metadata": {},
   "source": [
    "In practice, to remove these punctuation characters, we can simply iterate over the text and remove characters found in the list, such as shown below in the `remove_punct` function.\n",
    "\n",
    "**Question**: What is this function doing, exactly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237d868d-339d-4bbe-9a3b-20fa5fbdf231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    '''Remove punctuation marks in input text'''\n",
    "    \n",
    "    # Select characters not in puncutaion\n",
    "    no_punct = []\n",
    "    for char in text:\n",
    "        if char not in punctuation:\n",
    "            no_punct.append(char)\n",
    "\n",
    "    # Join the characters into a string\n",
    "    text_no_punct = ''.join(no_punct)   \n",
    "    \n",
    "    return text_no_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc768b-c2dd-4386-8212-483c4485e4be",
   "metadata": {},
   "source": [
    "Let's apply the function to the example below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7596c465-3d85-4b72-a853-f2151bcd91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the third example\n",
    "print(tweets['text'][20])\n",
    "print(f\"{'=' * 50}\")\n",
    "\n",
    "# Apply the function \n",
    "remove_punct(tweets['text'][20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853a4b83-f503-4405-aedd-66bbc088e3e7",
   "metadata": {},
   "source": [
    "Let's give it a try with another tweet. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c2f60-fc92-4326-bad6-5ad04be50476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print another tweet\n",
    "print(tweets['text'][100])\n",
    "print(f\"{'=' * 50}\")\n",
    "\n",
    "# Apply the function\n",
    "remove_punct(tweets['text'][100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af02ce5-b674-4cb4-8e08-7d7416963f9c",
   "metadata": {},
   "source": [
    "How about the following example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8c3947-e6b8-42fe-8a58-15e4b6c60005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a text with contraction\n",
    "contraction_text = \"We've got quite a bit of punctuation here, don't we?!? #Python @pbiscaye.\"\n",
    "\n",
    "# Apply the function\n",
    "remove_punct(contraction_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62574c66-db3f-4500-9c3b-cea2f3eb2a30",
   "metadata": {},
   "source": [
    "We have lost some potentially useful information. In many cases, we want to remove punctuation **after** tokenization, which we will discuss in the next section. \n",
    "\n",
    "This tells us that the **order** of preprocessing is a matter of importance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c159cb-8eaa-4c30-b8ff-38a712d2bb0f",
   "metadata": {},
   "source": [
    "## Task-specific Processes\n",
    "\n",
    "Now that we understand common preprocessing operations, there are still a few additional operations to consider. Our text data might require further normalization depending on the language, source, and content of the data.\n",
    "\n",
    "For example, if we are dealing with financial documents, we might want to standardize monetary symbols by converting them to digits. In the airline tweet data we've been using, there are numerous hashtags and URLs. These can be replaced with placeholders to simplify subsequent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2936cea-74e9-40c2-bfbe-6ba8129330de",
   "metadata": {},
   "source": [
    "### Example: Remove Hashtags and URLs \n",
    "\n",
    "Although URLs, hashtags, and numbers are informative in their own right, sometimes we may not necessarily care about the exact meaning of each of them. Again, all preprocessing depends on the goals of the text analysis.\n",
    "\n",
    "While we could remove them completely, it's often informative to know that there **exists** a URL or a hashtag. So, we replace individual URLs and hashtags with a \"symbol\" that preserves the fact these structures exist in the text. It's standard to just use the strings \"URL\" and \"HASHTAG\".\n",
    "\n",
    "Since these types of text often contain precise structure, they're an apt case for using regular expressions. Let's apply these patterns to the tweets data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c0dc37-a013-4f0a-b72f-a1f64dc6c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the example tweet \n",
    "tweets['text'][13]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2110f4d-8743-414e-bb84-06844a312358",
   "metadata": {},
   "source": [
    "**Question**: Can you guess what the below pattern is looking for? *Hint*: `\\w = [a-zA-Z0-9_]` and `+` means one or more characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef61bea-ea11-468d-8176-a2f63659d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL \n",
    "url_pattern = r'(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])'\n",
    "url_repl = ' URL '\n",
    "re.sub(url_pattern, url_repl, tweets['text'][13])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cbd703-968c-498c-ae6e-356e51737420",
   "metadata": {},
   "source": [
    "**Question**: Can you guess what the below pattern is looking for? *Hint*: `^` means the start of a string, and `\\s` is a whitespace character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8e0f2a-460e-4088-aa89-dc2a8bc6f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hashtag\n",
    "hashtag_pattern = r'(?:^|\\s)[＃#]{1}(\\w+)'\n",
    "hashtag_repl = ' HASHTAG '\n",
    "re.sub(hashtag_pattern, hashtag_repl, tweets['text'][13])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d68d49-4923-49c0-9113-b844dc7546b9",
   "metadata": {},
   "source": [
    "# 2. Tokenization\n",
    "\n",
    "## Tokenizers Before LLMs\n",
    "\n",
    "One of the most important steps in text analysis is tokenization. This is the process of breaking down a long sequence of text into pieces of word tokens. With these tokens available, we are ready to perform word-level analysis. For instance, we can filter out tokens that do not contribute to the core meaning of the text.\n",
    "\n",
    "In this section, we'll introduce how to perform tokenization with `nltk` and `spaCy`, as well as tokenization with a Large Language Model (`bert`). The purpose is to expose you to different NLP packages, understand what functionalities each of them provide, and how to access functions within each.\n",
    "\n",
    "### `nltk`\n",
    "\n",
    "The first package we'll be using is called **Natural Language Toolkit**, or `nltk`. \n",
    "\n",
    "Let's install a couple modules within the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441d81f8-361e-4273-bd36-91a272f4a38a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b327cc-5c77-4fdc-9aaf-17d7f0761237",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e79b699-c3a5-489f-9b3c-95653aba34d6",
   "metadata": {},
   "source": [
    "`nltk` has a function called `word_tokenize`, which tokenizes a string for us in an intelligent fashion. It requires one argument, which is the text to be tokenized, and then returns a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d6944-c641-4fac-a239-5947a496371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word_tokenize in\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Print the example\n",
    "print(tweets['text'][7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fde2a3-e4e2-4e61-ad54-e4d5d0a6ba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the NLTK tokenizer\n",
    "nltk_tokens = word_tokenize(tweets['text'][7])\n",
    "nltk_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ead039-7721-4b22-8590-0d7824631675",
   "metadata": {},
   "source": [
    "Here we are, with a list of tokens identified by `nltk`.\n",
    "\n",
    "**Question**: Do the word boundaries decided by `nltk` make sense to you? \n",
    "\n",
    "The function we used just now was imported from the `nltk.tokenize` module, which as the name suggests, primarily does the job of tokenization. This function also relies on some of the packages we downloaded above. \n",
    "\n",
    "`nltk` has [a collection of modules](https://www.nltk.org/api/nltk.html) that fulfill different purposes. For example: \n",
    "\n",
    "| NLTK module   | Function                  | Link                                                         |\n",
    "|---------------|---------------------------|--------------------------------------------------------------|\n",
    "| nltk.tokenize | Tokenization              | [Documentation](https://www.nltk.org/api/nltk.tokenize.html) |\n",
    "| nltk.corpus   | Retrieve built-in corpora | [Documentation](https://www.nltk.org/nltk_data/)             |\n",
    "| nltk.tag      | Part-of-speech tagging    | [Documentation](https://www.nltk.org/api/nltk.tag.html)      |\n",
    "| nltk.stem     | Stemming                  | [Documentation](https://www.nltk.org/api/nltk.stem.html)     |\n",
    "\n",
    "Let's import `stopwords` from the `nltk.corpus` module, which hosts various built-in corpora available in `nltk`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bbfced-8803-41ca-9cae-49bdadf8c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in predefined stop words from nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee971a1-1189-4cb6-8317-4836f54c3ae2",
   "metadata": {},
   "source": [
    "What are **stop words**? These are words considered as inconsequential for text analysis, with little value in helping processors answer queries. The specific stop words can vary based on context and the goals of the analysis.\n",
    "\n",
    "Let's specify that we want to retrieve English stop words. The function simply returns a list of stop words, mostly function words, that `nltk` identifies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6009e1df-b720-4d22-a162-7fd250a58672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 10 stopwords\n",
    "stop = stopwords.words('english')\n",
    "stop[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89760ee3-d1c3-49c8-bcaa-952a3756c44c",
   "metadata": {},
   "source": [
    "**Question**: What is your reaction to these stop words? When might we *not* want to strip them from our text? When might we not want them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3ec908-de6c-42c5-a370-f1b1df0032b3",
   "metadata": {},
   "source": [
    "### `spaCy`\n",
    "Other than `nltk`, we have another widely-used package called `spaCy`. \n",
    "\n",
    "`spaCy` has its own processing pipeline. It takes in a string of text, runs the `nlp` pipeline on it, and stores the processed text and its annotations in an object called `doc`. The `nlp` pipeline always performs tokenization, followed by [a number of components](https://spacy.io/usage/processing-pipelines#custom-components) as specified by the user. These components are, in fact, pretty similar to modules in `nltk`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a0facd-4b75-41ac-920c-5ea044f7ae2e",
   "metadata": {},
   "source": [
    "<img src='Images/spacy.png' alt=\"spacy pipeline\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ef1eaf-2790-4928-b094-943f2803c6a0",
   "metadata": {},
   "source": [
    "Note that it always starts by initializing a `nlp` pipeline, which will depend on the language of the text. Here, we will load a pretrained language model for English which you should have already installed: `en_core_web_sm`. It means the model is trained on a small set of web text data in English.\n",
    "\n",
    "This is the first time we encounter the concept of **pretraining**, though you may have heard it elsewhere. In the context of NLP, pretraining means that the tools we are using are trained on millions of English texts. As a result of pretraining, the model we import in already comes with certain \"knowledge\" of the structure and grammar of English texts. \n",
    "\n",
    "Therefore, when we apply the model to our own data, we can expect it to be somewhat (but not fully) accurate in performing various annotation tasks, e.g., tagging the part of speech of a word, identifying the syntactic head of a phrase, etc. \n",
    "\n",
    "Let's dive in! We'll first need to load the pretrained language model that we have installed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524dfc02-aa8f-4888-9f81-74a570db72b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d669c3-2f5a-41b6-893b-ea1d438b3a48",
   "metadata": {},
   "source": [
    "The `nlp` pipeline by default includes a set of components, which we can access via the `.pipe_names` attribute. \n",
    "\n",
    "You may notice that it doesn't include the tokenizer. Don't worry! Tokenizer is a special component that the pipeline always includes, thus it is not counted towards added components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d581ca5-43f8-4ef9-b099-2fc92c324581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve components included in NLP pipeline\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e37f91-d174-4101-bfc6-2859cb0fe5cc",
   "metadata": {},
   "source": [
    "Let's run the `nlp` pipeline on an example tweet, and assign it to a variable `doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618e8558-625d-4546-8109-63f9bae9790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the pipeline to example tweet\n",
    "doc = nlp(tweets['text'][7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54325d60-5c5c-488d-baf2-7eed4de2c031",
   "metadata": {},
   "source": [
    "Under the hood, the `doc` object contains the tokens (done by the tokenizer) and their annotations (done by other components), which are [linguistic features](\n",
    "https://spacy.io/usage/linguistic-features) useful for text analysis. We retrieve the token and its annotations by accessing corresponding attributes. \n",
    "\n",
    "| Attribute      | Annotation                              | Link                                                                      |\n",
    "|----------------|-----------------------------------------|---------------------------------------------------------------------------|\n",
    "| token.text     | The token itself                       | [Documentation](https://spacy.io/api/token#attributes)                    |\n",
    "| token.is_stop  | Whether the token is a stop word        | [Documentation](https://spacy.io/api/attributes#_title)                   |\n",
    "| token.is_punct | Whether the token is a punctuation mark | [Documentation](https://spacy.io/api/attributes#_title)                   |\n",
    "| token.lemma_   | The base form of the token              | [Documentation](https://spacy.io/usage/linguistic-features#lemmatization) |\n",
    "| token.pos_     | The simple POS-tag of the token         | [Documentation](https://spacy.io/usage/linguistic-features#pos-tagging)   |\n",
    "| ...            | ...                                     | ...                                                                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9f23c8-a157-44a7-a6ec-6894aec1a595",
   "metadata": {},
   "source": [
    "Let's first get the tokens themselves! We'll iterate over the `doc` object, and retrieve the verbatim text `token.text` for each token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0fcda5-b6fe-4935-8aa5-28aad021794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the verbatim texts of tokens\n",
    "spacy_tokens = [token.text for token in doc]\n",
    "spacy_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fc23f0-c699-45e6-ad62-e131036d601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the NLTK tokens\n",
    "nltk_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ace59e-40e0-42b3-9f2b-d30ac94dccab",
   "metadata": {},
   "source": [
    "**Question**: Compare the tokens from `nltk` and `spaCy`. What do you notice?\n",
    "\n",
    "Remember we can also access various annotations of these okens. For instance, one annotation `spaCy` offers is that it conveniently encodes whether a token is a stop word. \n",
    "\n",
    "These annotations really make it straightforward for us to do certain preprocessing tasks without needing to refer to functions in specific modules as we would with `nltk`. A challenge in the practice notebook will ask you to use these results to remove stop works from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626af687-e986-4c97-af86-edf7dbd22c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the is_stop annotation\n",
    "spacy_stops = [token.is_stop for token in doc]\n",
    "\n",
    "# The results are boolean values\n",
    "spacy_stops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a6b1ec-87cc-4a08-a5dd-0210a9c56f0b",
   "metadata": {},
   "source": [
    "## Example: Powerful Features from `spaCy`\n",
    "\n",
    "`spaCy`'s nlp pipeline includes a number of linguistic annotations which could be very useful for text analysis. \n",
    "\n",
    "For instance, we can access more annotations such as the lemma, the part-of-speech tag and its meaning, and whether the token looks like URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6c7d93-51a3-4fb8-8321-cb672f4f1b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print tokens and their annotations\n",
    "# Note that  < tells the text to be left aligned and the number specifies minimum width of the field in characters\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<24} | {token.lemma_:<24} | {token.pos_:<12} | {spacy.explain(token.pos_):<12} | {token.like_url:<12} |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17388e0c-88b6-4cd9-8d2b-adb7f10b5330",
   "metadata": {},
   "source": [
    "As you can imagine, it is typical for this dataset to contain place names and airport codes. It would be cool if we are able to identify them and extract them from tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb489f78-fbb2-497b-a36d-3400c00c9b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print example tweets with place names and airport codes\n",
    "tweet_city = tweets['text'][8273]\n",
    "tweet_airport = tweets['text'][15]\n",
    "print(tweet_city)\n",
    "print(f\"{'=' * 50}\")\n",
    "print(tweet_airport)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013990a5-5e07-4a45-9427-fcd33840d3b8",
   "metadata": {},
   "source": [
    "We can use the \"ner\" ([Named Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition)) component of the `nlp` pipeline to identify entities and their categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e63519-5991-49fa-9a5d-be9f9b408ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print entities identified from the text\n",
    "doc_city = nlp(tweet_city)\n",
    "for ent in doc_city.ents:\n",
    "    print(f\"{ent.text:<15} | {ent.start_char:<10} | {ent.end_char:<10} | {ent.label_:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b933ed0-7018-450c-b0a6-fb76cb6d5be9",
   "metadata": {},
   "source": [
    "`GPE` means \"geopolitical entities,\" usually things like country and city names.\n",
    "\n",
    "We can also use `displacy` to highlight entities identified in the text, and at the same time, annotate the entity category. \n",
    "\n",
    "In the following example, we have four GPE identified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5a5219-af1f-445c-a35b-7c49d739a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the identified entities\n",
    "from spacy import displacy\n",
    "displacy.render(doc_city, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d7953b-04a1-46d0-817a-db29edd8c83b",
   "metadata": {},
   "source": [
    "Let's give it a try with another example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246c7e3-8990-4d47-a355-deb63dbd1cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print entities identified from the text\n",
    "doc_airport = nlp(tweet_airport)\n",
    "for ent in doc_airport.ents:\n",
    "     print(f\"{ent.text:<15} | {ent.start_char:<10} | {ent.end_char:<10} | {ent.label_:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df38472-3193-44c5-9b2f-e311ce9d42e0",
   "metadata": {},
   "source": [
    "Interesting that airport codes are identified as `ORG`—organizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4bf382-4c57-4b78-a37f-fc1f6cb1d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the identified entities\n",
    "displacy.render(doc_airport, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7636f495-ef6b-42d9-9a53-5b22bbe7792c",
   "metadata": {},
   "source": [
    "**Question**: Can you identify any mistakes the `nlp` made in identifying entities here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467d6f28-effc-4fe1-90e9-47c89dc5492d",
   "metadata": {},
   "source": [
    "## Tokenizers Since LLMs\n",
    "\n",
    "So far, we've seen what tokenization looks like with two widely-used NLP packages. They work quite well in some settings, but not others. Recall that NLTK struggles with URLs for example. Now, imagine the data we have is even messier, containing misspellings, recently coined words, foreign names, etc, (collectively called \"out of vocabulary\" or OOV words). In such circumstances, we might need a more powerful model to handle these complexities.\n",
    "\n",
    "In fact, tokenization schemes have changed substantially with **Large Language Models** (LLMs), which are models trained on a vast amount of data from mixed sources and linguistic genres. With that magnitude of data, LLMs are become better at chunking a longer sequence into tokens and tokens into **subtokens**. Subtokens could be morphological units of a word, such as a prefix, but they could also be parts of a word where the model sets a \"meaningful\" boundary. \n",
    "\n",
    "In this section, we will demonstrate tokenization using **BERT** (Bidirectional Encoder Representations from Transformers), which utilizes a tokenization algorithm called [**WordPiece**](https://huggingface.co/learn/nlp-course/en/chapter6/6). \n",
    "\n",
    "We will load the tokenizer of BERT from the package `transformers`, which hosts a number of Transformer-based LLMs (e.g., GPT-2). We will not go into the architecture of Transformer in this section, but the D-lab workshop on [GPT Fundamentals](https://github.com/dlab-berkeley/GPT-Fundamentals) may be a helpful place to start if you are interested in learningmore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e1509b-5b9e-456d-909f-a6b5099c48c8",
   "metadata": {},
   "source": [
    "### WordPiece Tokenization\n",
    "\n",
    "Note that BERT comes in a variety of versions. The one we will explore today is `bert-base-uncased`. This model has a moderate size (referred to as `base`) and is case-insensitive, meaning the input text will be lowercased by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7927226c-d04e-4117-9c49-3d355611b209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load BERT tokenizer in\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128cfb38-274e-4d75-9d14-8744020fe49c",
   "metadata": {},
   "source": [
    "The tokenizer has multiple functions, as we will see in a minute. Right now we want to access the `.tokenize()` function from the tokenizer. \n",
    "\n",
    "Let's tokenize an example tweet below. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62649193-bb00-4ae8-9102-bce3d1dfb6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an example tweet from dataframe\n",
    "print(f\"Text: {tweets['text'][194]}\")\n",
    "print(f\"{'=' * 50}\")\n",
    "\n",
    "# Apply tokenizer\n",
    "tokens = tokenizer.tokenize(tweets['text'][194])\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4f780e-207c-4d3d-b1b6-063c6d118945",
   "metadata": {},
   "source": [
    "The double \"hashtag\" symbols (`##`) refer to a subword token—a segment chunked off from the previous token.\n",
    "\n",
    "**Question**: Do these subwords make sense to you? \n",
    "\n",
    "One significant development with LLMs is that each token is assigned an ID in its vocabulary. This is important because computational analysis does not operate directly on strings of text. Our computer does not understand text in its raw form, so each token is translated to an ID. These IDs are the inputs that the model can access and operate.\n",
    "\n",
    "Tokens and IDs can be converted bidirectionally, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3112a63d-82b6-4fc0-a904-ab86f8740653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the input ID of the word \n",
    "print(f\"ID of just is: {tokenizer.vocab['just']}\")\n",
    "\n",
    "# Get the text of the input ID\n",
    "print(f\"Token 2074 is: {tokenizer.decode([2074])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9fd13d-f26e-480c-b43e-2b5fbc4898cd",
   "metadata": {},
   "source": [
    "Let's convert tokens to input IDs and look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d125e1d-2560-4136-829c-b1c11e34636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a list of tokens to a list of input IDs\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"Number of input IDs: {len(input_ids)}\")\n",
    "print(f\"Input IDs of text: {input_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25a1a14-a6db-414b-a1a8-c6be73c81a15",
   "metadata": {},
   "source": [
    "### Special Tokens\n",
    "\n",
    "In addition to the tokens and subtokens discussed above, BERT also makes use of three special tokens: `SEP`, `CLS`, and `UNK`. The `SEP` token acts as a sentence terminator, commonly known as an `EOS` (End of Sentence) token. The `UNK` token represents any token that is not found in the vocabulary, hence \"unknown\" tokens. The `CLS` token is added to the beginning of the sentence. It originates from text classification tasks (e.g., spam detection), where reseachers found it useful to have a token that aggregates the information of the entire sentence for classification purposes.\n",
    "\n",
    "When we apply `tokenizer()` directly to our text data, we are asking BERT to **encode** the text for us. This involves multiple steps: \n",
    "- Tokenize the text\n",
    "- Add special tokens\n",
    "- Convert tokens to input IDs\n",
    "- Other model-specific processes\n",
    "  \n",
    "Let's print them out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21479c25-7a9a-4fac-ba09-1812575b8170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the input IDs by providing the key \n",
    "input_ids_from_tokenizer = tokenizer(tweets['text'][194])['input_ids']\n",
    "print(f\"Number of input IDs: {len(input_ids_from_tokenizer)}\")\n",
    "print(f\"IDs from tokenizer: {input_ids_from_tokenizer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6ea98e-7374-488a-a804-46cab166125c",
   "metadata": {},
   "source": [
    "It seems we have two more tokens added: 101 and 102. \n",
    "\n",
    "Let's convert them to text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a18dc1-ca0d-4d5b-8ac6-f56cb44bf8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert input IDs to texts\n",
    "print(f\"The 101st token: {tokenizer.convert_ids_to_tokens(101)}\")\n",
    "print(f\"The 102nd token: {tokenizer.convert_ids_to_tokens(102)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d4e10-e96c-432a-a6c9-2c992d00fcde",
   "metadata": {},
   "source": [
    "As you can see, our text example is now a list of vocabulary IDs. In addition to that, BERT adds the sentence terminator `SEP` and the beginning `CLS` token to the original text. BERT encodes the rest of texts likewise and afterwards they are ready for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0812a7-f033-46ed-bc7b-67109c369e6c",
   "metadata": {},
   "source": [
    "## Key Points\n",
    "\n",
    "* Preprocessing includes multiple steps, some of them are more common to text data regardlessly, and some are task-specific. \n",
    "* Both `nltk` and `spaCy` could be used for tokenization and stop word removal. The latter is more powerful in providing various linguistic annotations. \n",
    "* Tokenization works differently in BERT, which often involves breaking down a whole word into subwords. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
